- command: ask  --identify --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T12:55:36.619528'
- command: ask  --identify --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T12:56:08.956855'
- command: ask  --prompt=what is the meaning of quantum entanglement?
  destination_file: Raw/quantum-entanglement-quantum-entanglement-phenomenon-quantum-physics-where-particles-become.md
  source_file: null
  timestamp: '2024-09-02T12:56:29.721911'
- command: ask  --prompt=Create a table of top 10 macro factors which impact the stock
    markets. Add columns for nature of impact, scale of impact, and recent example.
  destination_file: Raw/macro-factor-nature-impact-scale-impact-recent-example-interest-rates.md
  source_file: null
  timestamp: '2024-09-02T12:57:46.687063'
- command: ask  --prompt=Create a table of top 10 macro factors which impact the stock
    markets. Add columns for nature of impact, scale of impact, and recent example.
  destination_file: Raw/macro-factor-nature-impact-scale-impact-recent-example-interest-rates.md
  source_file: null
  timestamp: '2024-09-02T12:59:32.093227'
- command: ask  --prompt=Create a table of top 10 macro factors which impact the stock
    markets. Add columns for nature of impact, scale of impact, and recent example.
  destination_file: Raw/macro-factor-nature-impact-scale-impact-recent-example-interest-rates.md
  source_file: null
  timestamp: '2024-09-02T13:03:57.935359'
- command: ask  --prompt=Create a table of top 10 macro factors which impact the stock
    markets. Add columns for nature of impact, scale of impact, and recent example.
  destination_file: Raw/macro-factor-nature-impact-scale-impact-recent-example-interest-rates.md
  source_file: null
  timestamp: '2024-09-02T13:04:34.576559'
- command: ask  --prompt=Create a table of top 10 macro factors which impact the stock
    markets. Add columns for nature of impact, scale of impact, and recent example.
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:09:25.638996'
- command: ask  --prompt=Describe quantum computing in 10 paragraphs
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:10:02.634462'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:12:30.834925'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:12:54.226397'
- command: ask  --prompt=Describe quantum computing in 10 paragraphs
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:13:04.245409'
- command: ask  --prompt=Create a table of top 10 macro factors which impact the stock
    markets. Add columns for nature of impact, scale of impact, and recent example.
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:13:15.129434'
- command: ask  --prompt=Create a table of top 10 macro factors which impact the stock
    markets. Add columns for nature of impact, scale of impact, and recent example.
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:13:26.759337'
- command: ask  --prompt=Create a table of top 10 macro factors which impact the stock
    markets. Add columns for nature of impact, scale of impact, and recent example.
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:13:39.312212'
- command: ask  --prompt=Describe quantum computing in 10 paragraphs
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:13:45.886424'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T13:14:06.339919'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:13:36.238651'
- command: ask  --identify --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:13:47.705110'
- command: ask  --identify --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:14:05.881516'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:14:09.212130'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:15:22.173986'
- command: ask  --prompt=write a python function for fibonacci
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:15:37.371834'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:17:14.478803'
- command: ask  --prompt=write a python function for fibonacci
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:17:22.623358'
- command: ask  --prompt=write a python function for fibonacci
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:18:20.988403'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:18:27.580355'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth, and ROCE
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:24:25.941193'
- command: ask  --prompt=write a python function for fibonacci
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:24:42.723073'
- command: ask  --prompt=write a python function for fibonacci
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:25:40.201872'
- command: ask  --prompt=create a table of top 5 companies by ROCE with columns for
    stock symbol, stock 5-year growth, and ROCE
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T14:25:48.614813'
- command: vision  --path=Images/london-skyline.webp --display --prompt=Name the tall
    buildings --url=None
  destination_file: Vision/buildings-image-include-shard-gherkin-walkie-talkie-fenchurch-street-cheesegrater.webp
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-02T14:26:09.054924'
- command: expand  --prompt=List trends and insights related to LLMs --section=transcript
    --document=None
  destination_file: Posts/Anthropic CEO upstream interview expanded.md
  source_file: Raw/Anthropic CEO upstream interview.md
  timestamp: '2024-09-02T16:45:33.017264'
- command: expand  --prompt=List trends and insights related to LLMs --section=transcript
    --document=None
  destination_file: Posts/Anthropic CEO upstream interview expanded.md
  source_file: Raw/Anthropic CEO upstream interview.md
  timestamp: '2024-09-02T16:48:09.906912'
- command: expand  --prompt=List trends and insights related to LLMs --section=transcript
    --document=None
  destination_file: Posts/Anthropic CEO upstream interview expanded.md
  source_file: Raw/Anthropic CEO upstream interview.md
  timestamp: '2024-09-02T16:49:54.330747'
- command: ask  --prompt=create a table of planets in the solar system with size,
    distance from sun, day length, moons, etc.
  destination_file: null
  source_file: null
  timestamp: '2024-09-02T19:47:19.818748'
- command: expand  --prompt=List trends and insights related to LLMs --section=transcript
    --document=None
  destination_file: Posts/Anthropic CEO upstream interview expanded.md
  source_file: Raw/Anthropic CEO upstream interview.md
  timestamp: '2024-09-02T19:56:47.493088'
- command: expand  --section=post --document=None --prompt=None
  destination_file: Posts/Anthropic CEO upstream interview expanded expanded.md
  source_file: Posts/Anthropic CEO upstream interview expanded.md
  timestamp: '2024-09-02T20:02:56.050812'
- command: expand  --section=sections --document=None --prompt=None
  destination_file: Posts/Anthropic CEO upstream interview researched expanded.md
  source_file: Posts/Anthropic CEO upstream interview researched.md
  timestamp: '2024-09-02T20:26:52.314449'
- command: expand  --section=sections --document=None --prompt=None
  destination_file: Posts/Anthropic CEO upstream interview researched expanded.md
  source_file: Posts/Anthropic CEO upstream interview researched.md
  timestamp: '2024-09-02T20:30:07.571241'
- command: expand  --prompt=Create a numbered list of single sentence startup growth
    hack tips --section=transcript --document=None
  destination_file: Posts/Pieter Levels Startup Hacks expanded.md
  source_file: Raw/Pieter Levels Startup Hacks.md
  timestamp: '2024-09-03T07:16:53.433972'
- command: expand  --section=sections --document=None --prompt=None
  destination_file: Posts/Startup Growth Hacks expanded.md
  source_file: Posts/Startup Growth Hacks.md
  timestamp: '2024-09-03T07:21:45.241204'
- command: expand  --section=sections --document=None --prompt=None
  destination_file: Posts/Startup Growth Hacks expanded.md
  source_file: Posts/Startup Growth Hacks.md
  timestamp: '2024-09-03T07:25:00.007524'
- command: expand  --section=sections --document=None --prompt=None
  destination_file: Posts/Startup Growth Hacks expanded.md
  source_file: Posts/Startup Growth Hacks.md
  timestamp: '2024-09-03T07:28:07.311923'
- command: expand  --section=sections --document=None --prompt=None
  destination_file: Posts/Startup Growth Hacks expanded.md
  source_file: Posts/Startup Growth Hacks.md
  timestamp: '2024-09-03T07:32:13.138952'
- command: expand  --section=sections --document=None --prompt=None
  destination_file: Posts/Startup Growth Hacks expanded.md
  source_file: Posts/Startup Growth Hacks.md
  timestamp: '2024-09-03T07:34:09.849828'
- command: expand  --section=sections --document=None --prompt=None
  destination_file: Posts/Startup Growth Hacks expanded.md
  source_file: Posts/Startup Growth Hacks.md
  timestamp: '2024-09-03T07:35:16.562295'
- command: ask  --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T04:08:57.627949'
- command: ask  --prompt=what is the distance to moon
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T08:31:03.309503'
- command: ask  --file --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T08:33:27.791000'
- command: ask  --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T08:33:43.896687'
- command: ask  --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T08:34:33.758439'
- command: ask  --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T17:19:15.493769'
- command: ask  --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T17:19:59.490036'
- command: ask  --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T17:22:40.859161'
- command: ask  --file --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T17:22:59.963284'
- command: ask  --file --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T17:25:27.269980'
- command: ask  --file --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T17:26:39.598465'
- command: ask  --prompt=what is the distance to moon
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T18:00:28.846648'
- command: ask  --file --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T18:11:10.975042'
- command: ask  --file --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T18:13:25.322981'
- command: ask  --prompt=None
  destination_file: null
  source_file: null
  timestamp: '2024-09-04T18:15:20.501739'
- command: ask  --file --prompt=None
  destination_file: null
  prompt: Create a table of stocks with largest market cap in NASDAQ. Add columns
    for company, stock symbol, current stock price, P/E Ratio, Market Cap.
  source_file: null
  timestamp: '2024-09-04T18:19:16.017213'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Table of Top Stocks.md
  source_file: null
  timestamp: '2024-09-04T18:21:03.137176'
- command: ask  --prompt=what is the distance to moon
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-04T18:21:33.767438'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Table of Top Stocks.md
  source_file: null
  timestamp: '2024-09-04T18:29:53.774482'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-04T19:06:03.331513'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-04T19:08:15.896007'
- command: ask  --file --prompt=None
  destination_file: Raw/analysis-overview-company-nvidia-corporation-leading-technology-company-primarily-known.md
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-04T19:09:27.695829'
- command: ask  --file --prompt=None
  destination_file: Raw/analysis-overview-company-apple-multinational-technology-company-designs-manufactures-markets.md
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-04T19:13:49.533602'
- command: ask  --prompt=what is the currency of USA
  destination_file: Raw/currency-united-states-dollar.md
  prompt_file: null
  source_file: null
  timestamp: '2024-09-04T19:59:49.410230'
- command: ask  --prompt=what is the currency of USA
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-04T20:00:45.370889'
- command: ask  --prompt=How many INR is The currency of the USA is the United States
    Dollar (USD).
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-04T20:00:46.424562'
- command: ask  --prompt=what is the distance to moon
  destination_file: Raw/distance-between-earth-constant-orbits-earth-elliptical-however-average-distance.md
  prompt_file: null
  source_file: null
  timestamp: '2024-09-06T05:50:18.033533'
- command: ask  --prompt=what is the distance to moon
  destination_file: Raw/average-distance-between-earth-approximately-kilometers-miles-facts-about-distance.md
  prompt_file: null
  source_file: null
  timestamp: '2024-09-06T05:51:01.213802'
- command: ask  --prompt=what is the distance to moon
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-06T09:37:28.280660'
- command: ask  --prompt=what is the distance to moon
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-06T09:38:45.586602'
- command: ask  --prompt=what is the distance to moon
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-06T09:44:11.107371'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Table of Top Stocks.md
  source_file: null
  timestamp: '2024-09-06T09:44:33.567846'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-06T09:44:44.885098'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-06T21:50:42.177971'
- command: ask  --prompt=what is the current value of NVDA?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-06T21:51:17.664495'
- command: ask  --prompt=what is the current value of NVDA?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-06T22:09:49.570092'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-06T22:10:34.852443'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Table of Top Stocks.md
  source_file: null
  timestamp: '2024-09-06T22:11:52.304069'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Table of Planets.md
  source_file: null
  timestamp: '2024-09-06T22:42:25.932789'
- command: ask  --file --prompt=None
  destination_file: null
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-06T22:46:42.962809'
- command: ask  --file --prompt=None
  destination_file: Raw/analysis-brief-overview-company-primary-business-nvidia-corporation-leading-technology.md
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-06T22:51:35.970806'
- command: ask  --file --prompt=None
  destination_file: Posts/analysis-brief-overview-company-primary-business-apple-multinational-technology-company.md
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-06T22:55:56.268098'
- command: ask  --prompt=what is the distance from Webb to Earth?
  destination_file: Posts/james-space-telescope-located-second-lagrange-point-which-approximately-million.md
  prompt_file: null
  source_file: null
  timestamp: '2024-09-07T09:42:18.722869'
- command: ask  --prompt=what are the latest polls stating?
  destination_file: Posts/unable-provide-latest-polls-please-check-reliable-source-polling-website.md
  prompt_file: null
  source_file: null
  timestamp: '2024-09-07T09:59:09.884531'
- command: ask  --prompt=what are the latest polls stating?
  destination_file: Posts/latest-polls-provide-various-insights-presidential-election-points-national-polling.md
  prompt_file: null
  source_file: null
  timestamp: '2024-09-07T09:59:56.754768'
- command: ask  --prompt=what are the latest polls stating?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-07T10:01:00.958086'
- command: ask  --prompt=what are the latest polls stating?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-07T10:01:33.701220'
- command: ask  --prompt=what are the latest polls stating about Harris vs Trump?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-07T10:02:45.361797'
- command: ask  --prompt=what is the latest news about NVDA stock?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-07T10:05:59.605879'
- command: ask  --prompt=What is the current price of NVDA stock?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-07T10:06:35.121669'
- command: expand  --prompt=Summarize key recommendations --section=transcript --document=None
  destination_file: Posts/RawAnthropic Prompt Engineering.md expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T10:18:48.908070'
- command: expand  --section=transcript --document=None --prompt=None
  destination_file: Posts/RawAnthropic Prompt Engineering.md expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T10:34:02.539315'
- command: expand  --section=transcript --document=None --prompt=None
  destination_file: Posts/Anthropic Prompt Engineering.md expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T10:41:30.341509'
- command: expand  --section=transcript --document=None --prompt=None
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T10:43:29.747232'
- command: expand  --section=transcript --document=None --prompt=None
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T10:53:58.212073'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a numbered list of prompt engineering tips based on the given
    transcript. Rank these by most important to least, top to bottom.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T11:00:38.842341'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a numbered list of prompt engineering tips based on the given
    transcript. Rank these by most important to least, top to bottom.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T13:57:44.677040'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a numbered list of prompt engineering tips based on the given
    transcript. Rank these by most important to least, top to bottom.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T14:52:05.833508'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a numbered list of prompt engineering tips based on the given
    transcript. Rank these by most important to least, top to bottom.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T14:53:35.497335'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a numbered list of prompt engineering tips based on the given
    transcript. Rank these by most important to least, top to bottom.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T14:54:21.053495'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: null
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T15:19:52.182732'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a numbered list of prompt engineering tips based on the given
    transcript. Rank these by most important to least, top to bottom.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T15:23:37.734647'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a table comparing prompt engineering for the enterprise, for
    research, and prompting ChatGPT or Claude.AI fchatbot.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T20:43:03.096652'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a table comparing prompt engineering for the enterprise, for
    research, and prompting ChatGPT or Claude.AI chatbot.       e
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T20:46:00.317034'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a table comparing prompt engineering for the enterprise, for
    research, and prompting ChatGPT or Claude.AI chatbot.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T20:47:08.221373'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a table comparing prompt engineering for the enterprise, for
    research, and prompting ChatGPT or Claude.AI chatbot.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T20:48:12.921655'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Compare prompt engineering for enterprise, for research and for consumer
    use.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T20:55:26.495319'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Compare prompt engineering for enterprise, for research and for consumer
    use.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T20:57:22.411580'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Create a well structured article explaining differences between prompt
    engineering for enterprise, for research, and for consumer use cases. Create a
    side by side comparison table in the appropriate section. Reseach related material
    and provide citations.
  destination_file: Posts/Anthropic Prompt Engineering expanded.md
  prompt_file: null
  source_file: Raw/Anthropic Prompt Engineering.md
  timestamp: '2024-09-07T23:08:36.832365'
- command: ask  --prompt=what is the PEG ratio for NVDA?
  custom_prompt: what is the PEG ratio for NVDA?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T07:29:32.817716'
- command: expand  --section=transcript --document=None --prompt=None
  custom_prompt: Summarize this
  destination_file: Posts/Elon Musk Design Process expanded.md
  prompt_file: null
  source_file: Raw/Elon Musk Design Process.md
  timestamp: '2024-09-08T07:49:17.270111'
- command: ask  --prompt=None
  custom_prompt: What is the name of the last US president?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T08:16:54.477061'
- command: ask  --prompt=what is the PEG ratio for NVDA?
  custom_prompt: what is the PEG ratio for NVDA?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T08:17:29.275629'
- command: ask  --file --prompt=None
  custom_prompt: Create a table of planets in the solar system. Add columns for size,
    moons, distance from sun, gravity, composition.
  destination_file: null
  prompt_file: Prompts/Table of Planets.md
  source_file: null
  timestamp: '2024-09-08T08:17:38.701015'
- command: ask  --file --prompt=None
  custom_prompt: "You are an expert stock analyst tasked with providing a detailed\
    \ analysis and recommendation for a given stock. You will need to conduct thorough\
    \ research and think step by step to provide a comprehensive response. Follow\
    \ these instructions carefully:\n\n1. You will be analyzing the stock with the\
    \ following symbol:\n<stock_symbol>{{MSFT}}</stock_symbol>\n\n2. Begin by reading\
    \ the most recent SEC filings for the company associated with this stock symbol.\
    \ Focus on the 10-K and 10-Q reports. Pay close attention to financial statements,\
    \ management's discussion and analysis, and any risk factors mentioned.\n\n3.\
    \ Next, search for and read recent media reports about the company, particularly\
    \ those that have impacted the stock price positively or negatively. Look for\
    \ news about earnings reports, product launches, management changes, legal issues,\
    \ or any other significant events.\n\n4. Research the industry to which this company\
    \ belongs. Read industry reports and analyze macro factors that could impact the\
    \ stock. Consider economic trends, regulatory changes, technological advancements,\
    \ and competitive landscape.\n\n5. Using the insights gathered from the previous\
    \ steps, prepare a detailed analysis of the company's stock. Your analysis should\
    \ include:\n\n   A. A brief overview of the company and its primary business\n\
    \   B. Key financial metrics and their trends\n   C. Strengths and weaknesses\
    \ of the company\n   D. Opportunities and threats in the market\n   E. Competitive\
    \ position within the industry\n   F. Potential catalysts for stock price movement\n\
    \   G. Table of key stock indicators including Market Cap, ROCE, P/E Ratio, Free\
    \ Cash Flow, etc.\n\n6. Based on your analysis, provide a recommendation of either\
    \ BUY, HOLD, or SELL for the stock. Include a detailed rationale for your recommendation,\
    \ citing specific factors from your research that support your conclusion.\n\n\
    7. Present your findings in the following markdown format:\n\n# Analysis\n[Your\
    \ detailed analysis goes here, covering points A-G from step 5]\n\n# Recommendation\n\
    Recommendation: [BUY/HOLD/SELL]\n\n## Rationale:\n[Provide a thorough explanation\
    \ of your recommendation, referencing key points from your analysis]\n\nRemember\
    \ to think critically and consider multiple perspectives before making your recommendation.\
    \ Your analysis should be objective, well-reasoned, and supported by the information\
    \ you've gathered."
  destination_file: null
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-08T08:17:58.306757'
- command: ask  --prompt=None
  custom_prompt: What is the distance to Webb?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T08:18:48.493201'
- command: ask  --prompt=what is the PEG ratio for NVDA? --template=None
  custom_prompt: what is the PEG ratio for NVDA?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T10:04:17.541626'
- command: ask  --prompt=None --template=None
  custom_prompt: Create a table of planets in the solar system. Add columns for size,
    moons, distance from sun, gravity, composition.
  destination_file: null
  prompt_file: Prompts/Table of Planets.md
  source_file: null
  timestamp: '2024-09-08T10:04:23.961668'
- command: ask  --prompt=None --template=None
  custom_prompt: "You are an expert stock analyst tasked with providing a detailed\
    \ analysis and recommendation for a given stock. You will need to conduct thorough\
    \ research and think step by step to provide a comprehensive response. Follow\
    \ these instructions carefully:\n\n1. You will be analyzing the stock with the\
    \ following symbol:\n<stock_symbol>{{AAPL}}</stock_symbol>\n\n2. Begin by reading\
    \ the most recent SEC filings for the company associated with this stock symbol.\
    \ Focus on the 10-K and 10-Q reports. Pay close attention to financial statements,\
    \ management's discussion and analysis, and any risk factors mentioned.\n\n3.\
    \ Next, search for and read recent media reports about the company, particularly\
    \ those that have impacted the stock price positively or negatively. Look for\
    \ news about earnings reports, product launches, management changes, legal issues,\
    \ or any other significant events.\n\n4. Research the industry to which this company\
    \ belongs. Read industry reports and analyze macro factors that could impact the\
    \ stock. Consider economic trends, regulatory changes, technological advancements,\
    \ and competitive landscape.\n\n5. Using the insights gathered from the previous\
    \ steps, prepare a detailed analysis of the company's stock. Your analysis should\
    \ include:\n\n   A. A brief overview of the company and its primary business\n\
    \   B. Key financial metrics and their trends\n   C. Strengths and weaknesses\
    \ of the company\n   D. Opportunities and threats in the market\n   E. Competitive\
    \ position within the industry\n   F. Potential catalysts for stock price movement\n\
    \   G. Table of key stock indicators including Market Cap, ROCE, P/E Ratio, Free\
    \ Cash Flow, etc.\n\n6. Based on your analysis, provide a recommendation of either\
    \ BUY, HOLD, or SELL for the stock. Include a detailed rationale for your recommendation,\
    \ citing specific factors from your research that support your conclusion.\n\n\
    7. Present your findings in the following markdown format:\n\n# Analysis\n[Your\
    \ detailed analysis goes here, covering points A-G from step 5]\n\n# Recommendation\n\
    Recommendation: [BUY/HOLD/SELL]\n\n## Rationale:\n[Provide a thorough explanation\
    \ of your recommendation, referencing key points from your analysis]\n\nRemember\
    \ to think critically and consider multiple perspectives before making your recommendation.\
    \ Your analysis should be objective, well-reasoned, and supported by the information\
    \ you've gathered."
  destination_file: null
  prompt_file: Prompts/Stock Analysis.md
  source_file: null
  timestamp: '2024-09-08T10:04:36.391470'
- command: ask  --prompt=prompt --template=None
  custom_prompt: prompt
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T10:05:27.683571'
- command: ask  --prompt=prompt --template=None
  custom_prompt: prompt
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T10:06:23.611421'
- command: ask  --prompt=template=Table of Planets --template=None
  custom_prompt: template=Table of Planets
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T10:07:31.555793'
- command: ask  --prompt=prompt= --template=None
  custom_prompt: prompt=
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T10:08:30.225543'
- command: ask  --prompt=what is the PEG ratio for NVDA? --template=None
  custom_prompt: what is the PEG ratio for NVDA?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T10:08:46.496141'
- command: ask  --prompt=what is the PEG ratio for NVDA? --template=None
  custom_prompt: what is the PEG ratio for NVDA?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T10:21:58.874316'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T11:19:20.891786'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T12:14:37.534137'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-08T12:15:09.013173'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/image-shows-large-group-photo-taken-aerial-perspective-depicts-circular.jpg
  prompt_file: null
  source_file: Images/hackathon.jpg
  timestamp: '2024-09-08T12:19:20.051567'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/image-showcases-stunning-aerial-london-iconic-skyline-elements-include-tower.webp
  prompt_file: null
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-08T12:27:07.815533'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T12:35:25.081692'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T12:36:33.032945'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/.md.webp
  prompt_file: null
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-08T12:36:48.041213'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/image-shows-group-people-gathered-appears-outdoor-standing-around-posing.jpg
  prompt_file: null
  source_file: Images/hackathon.jpg
  timestamp: '2024-09-08T12:37:27.610422'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/image-depicts-stunning-aerial-london-iconic-tower-bridge-prominently-featured.webp
  prompt_file: null
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-08T12:38:12.573541'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/aerial-london-night-river-thames-foreground-tower-bridge-several-other.webp
  prompt_file: null
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-08T12:44:20.713463'
- command: refer  --section=transcript --document=None --prompt=None
  custom_prompt: Summarize this
  destination_file: Posts/Automating video creation updated.md
  prompt_file: null
  source_file: Raw/Automating video creation.md
  timestamp: '2024-09-08T13:02:03.839905'
- command: refer  --section=post --document=None --prompt=None
  custom_prompt: Expand the contents of this post will well researched sources.
  destination_file: Posts/Automating video creation updated updated.md
  prompt_file: null
  source_file: Posts/Automating video creation updated.md
  timestamp: '2024-09-08T13:20:10.731093'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/.md.webp
  prompt_file: null
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-08T16:39:23.607709'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/.md.jpg
  prompt_file: null
  source_file: Images/hackathon.jpg
  timestamp: '2024-09-08T16:39:57.288298'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/image-photo-group-people-appear-organization-suggested-matching-shirts-wearing.jpg
  prompt_file: null
  source_file: Images/hackathon.jpg
  timestamp: '2024-09-08T16:40:38.337804'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/.md.webp
  prompt_file: null
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-08T16:40:50.312153'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: Images/london-skyline.webp
  timestamp: '2024-09-08T16:43:30.211993'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: Vision/image-shows-group-people-gathered-together-wearing-shirts-design-standing.jpg
  prompt_file: null
  source_file: Images/hackathon.jpg
  timestamp: '2024-09-08T16:43:39.720048'
- command: vision  --camera --display --prompt=what does this gesture mean --path=None
    --url=None
  custom_prompt: null
  destination_file: Vision/gesture-shown-image-commonly-known-thumbs-generally-signifies-approval-agreement.jpg
  prompt_file: null
  source_file: Camera
  timestamp: '2024-09-08T16:48:23.695696'
- command: vision  --prompt=None --path=None --url=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T16:57:06.542633'
- command: refer  --section=intents --document=None --prompt=None
  custom_prompt: null
  destination_file: Intents/Event Planning updated.md
  prompt_file: null
  source_file: Intents/Event Planning.md
  timestamp: '2024-09-08T18:36:09.330359'
- command: intents  --document=None
  custom_prompt: null
  destination_file: Embeds/Payment Methods for Event Planning Startup.md
  prompt_file: null
  source_file: Intents/Event Planning updated.md
  timestamp: '2024-09-08T18:38:43.688268'
- command: intents  --document=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T18:56:01.327052'
- command: intents  --document=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T18:56:20.555523'
- command: intents  --document=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T18:56:27.762449'
- command: intents  --document=None
  custom_prompt: null
  destination_file: Embeds/Macro Factors Impact Stocks.md
  prompt_file: null
  source_file: Intents/Financial Analysis.md
  timestamp: '2024-09-08T19:06:00.153007'
- command: refer  --section=intents --document=None --prompt=None
  custom_prompt: null
  destination_file: Intents/Event Planning updated.md
  prompt_file: null
  source_file: Intents/Event Planning.md
  timestamp: '2024-09-08T19:06:37.617785'
- command: intents  --document=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T19:08:26.894385'
- command: refer  --section=intents --document=None --prompt=None
  custom_prompt: null
  destination_file: Intents/Event Planning updated updated.md
  prompt_file: null
  source_file: Intents/Event Planning updated.md
  timestamp: '2024-09-08T19:08:43.510734'
- command: intents  --document=None
  custom_prompt: null
  destination_file: Embeds/Event Planning Portfolio Development.md
  prompt_file: null
  source_file: Intents/Event Planning updated updated.md
  timestamp: '2024-09-08T19:09:28.967210'
- command: intents  --document=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T19:18:38.819587'
- command: ask  --prompt=what is the PEG ratio for NVDA? --template=None
  custom_prompt: what is the PEG ratio for NVDA?
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T19:31:32.622526'
- command: intents  --document=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T22:21:52.324228'
- command: intents  --document=None
  custom_prompt: null
  destination_file: null
  prompt_file: null
  source_file: null
  timestamp: '2024-09-08T22:23:07.430035'
- command: intents  --document=None
  custom_prompt: null
  destination_file: Embeds/Top Companies by ROCE.md
  prompt_file: null
  source_file: Intents/Financial Analysis.md
  timestamp: '2024-09-08T22:23:49.323957'
- command: refer  --section=transcript --document=None --prompt=None
  custom_prompt: List prompt examples shared in the paper
  destination_file: Posts/A Survey of Large Language Models updated.md
  prompt_file: null
  source_file: Raw/A Survey of Large Language Models.md
  timestamp: '2024-09-08T23:33:49.893256'
- command: ask  --prompt=None --template=None
  custom_prompt: "Read the entire text and identify tips, techniques, recommendations,\
    \ and insights. Classify these into sections as headings. List these within the\
    \ relevant sections using numbered list, ranking by order of importance in each\
    \ section. Each item should have a short phrase in bold introducing the item and\
    \ one sentence describing it.\n\n{{[\n\n# Prompt engineering\n\n](https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering)\n\
    \nThis guide shares strategies and tactics for getting better results from large\
    \ language models (sometimes referred to as GPT models) like GPT-4o. The methods\
    \ described here can sometimes be deployed in combination for greater effect.\
    \ We encourage experimentation to find the methods that work best for you.\n\n\
    You can also explore example prompts which showcase what our models are capable\
    \ of:\n\n[\n\nPrompt examples\n\nExplore prompt examples to learn what GPT models\
    \ can do\n\n\n\n\n\n\n\n](https://platform.openai.com/examples)\n\n[\n\n## Six\
    \ strategies for getting better results\n\n](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results)\n\
    \n[\n\n### Write clear instructions\n\n](https://platform.openai.com/docs/guides/prompt-engineering/write-clear-instructions)\n\
    \nThese models can\u2019t read your mind. If outputs are too long, ask for brief\
    \ replies. If outputs are too simple, ask for expert-level writing. If you dislike\
    \ the format, demonstrate the format you\u2019d like to see. The less the model\
    \ has to guess at what you want, the more likely you\u2019ll get it.\n\nTactics:\n\
    \n- [Include details in your query to get more relevant answers](https://platform.openai.com/docs/guides/prompt-engineering/tactic-include-details-in-your-query-to-get-more-relevant-answers)\n\
    - [Ask the model to adopt a persona](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-to-adopt-a-persona)\n\
    - [Use delimiters to clearly indicate distinct parts of the input](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input)\n\
    - [Specify the steps required to complete a task](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-steps-required-to-complete-a-task)\n\
    - [Provide examples](https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples)\n\
    - [Specify the desired length of the output](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-desired-length-of-the-output)\n\
    \n[\n\n### Provide reference text\n\n](https://platform.openai.com/docs/guides/prompt-engineering/provide-reference-text)\n\
    \nLanguage models can confidently invent fake answers, especially when asked about\
    \ esoteric topics or for citations and URLs. In the same way that a sheet of notes\
    \ can help a student do better on a test, providing reference text to these models\
    \ can help in answering with fewer fabrications.\n\nTactics:\n\n- [Instruct the\
    \ model to answer using a reference text](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text)\n\
    - [Instruct the model to answer with citations from a reference text](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-with-citations-from-a-reference-text)\n\
    \n[\n\n### Split complex tasks into simpler subtasks\n\n](https://platform.openai.com/docs/guides/prompt-engineering/split-complex-tasks-into-simpler-subtasks)\n\
    \nJust as it is good practice in software engineering to decompose a complex system\
    \ into a set of modular components, the same is true of tasks submitted to a language\
    \ model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore,\
    \ complex tasks can often be re-defined as a workflow of simpler tasks in which\
    \ the outputs of earlier tasks are used to construct the inputs to later tasks.\n\
    \nTactics:\n\n- [Use intent classification to identify the most relevant instructions\
    \ for a user query](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query)\n\
    - [For dialogue applications that require very long conversations, summarize or\
    \ filter previous dialogue](https://platform.openai.com/docs/guides/prompt-engineering/tactic-for-dialogue-applications-that-require-very-long-conversations-summarize-or-filter-previous-dialogue)\n\
    - [Summarize long documents piecewise and construct a full summary recursively](https://platform.openai.com/docs/guides/prompt-engineering/tactic-summarize-long-documents-piecewise-and-construct-a-full-summary-recursively)\n\
    \n[\n\n### Give the model time to \"think\"\n\n](https://platform.openai.com/docs/guides/prompt-engineering/give-the-model-time-to-think)\n\
    \nIf asked to multiply 17 by 28, you might not know it instantly, but can still\
    \ work it out with time. Similarly, models make more reasoning errors when trying\
    \ to answer right away, rather than taking time to work out an answer. Asking\
    \ for a \"chain of thought\" before an answer can help the model reason its way\
    \ toward correct answers more reliably.\n\nTactics:\n\n- [Instruct the model to\
    \ work out its own solution before rushing to a conclusion](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion)\n\
    - [Use inner monologue or a sequence of queries to hide the model's reasoning\
    \ process](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-inner-monologue-or-a-sequence-of-queries-to-hide-the-model-s-reasoning-process)\n\
    - [Ask the model if it missed anything on previous passes](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-if-it-missed-anything-on-previous-passes)\n\
    \n[\n\n### Use external tools\n\n](https://platform.openai.com/docs/guides/prompt-engineering/use-external-tools)\n\
    \nCompensate for the weaknesses of the model by feeding it the outputs of other\
    \ tools. For example, a text retrieval system (sometimes called RAG or retrieval\
    \ augmented generation) can tell the model about relevant documents. A code execution\
    \ engine like OpenAI's Code Interpreter can help the model do math and run code.\
    \ If a task can be done more reliably or efficiently by a tool rather than by\
    \ a language model, offload it to get the best of both.\n\nTactics:\n\n- [Use\
    \ embeddings-based search to implement efficient knowledge retrieval](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval)\n\
    - [Use code execution to perform more accurate calculations or call external APIs](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-code-execution-to-perform-more-accurate-calculations-or-call-external-apis)\n\
    - [Give the model access to specific functions](https://platform.openai.com/docs/guides/prompt-engineering/tactic-give-the-model-access-to-specific-functions)\n\
    \n[\n\n### Test changes systematically\n\n](https://platform.openai.com/docs/guides/prompt-engineering/test-changes-systematically)\n\
    \nImproving performance is easier if you can measure it. In some cases a modification\
    \ to a prompt will achieve better performance on a few isolated examples but lead\
    \ to worse overall performance on a more representative set of examples. Therefore\
    \ to be sure that a change is net positive to performance it may be necessary\
    \ to define a comprehensive test suite (also known an as an \"eval\").\n\nTactic:\n\
    \n- [Evaluate model outputs with reference to gold-standard answers](https://platform.openai.com/docs/guides/prompt-engineering/tactic-evaluate-model-outputs-with-reference-to-gold-standard-answers)\n\
    \n[\n\n## Tactics\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactics)\n\
    \nEach of the strategies listed above can be instantiated with specific tactics.\
    \ These tactics are meant to provide ideas for things to try. They are by no means\
    \ fully comprehensive, and you should feel free to try creative ideas not represented\
    \ here.\n\n[\n\n### Strategy: Write clear instructions\n\n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions)\n\
    \n[\n\n#### Tactic: Include details in your query to get more relevant answers\n\
    \n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-include-details-in-your-query-to-get-more-relevant-answers)\n\
    \nIn order to get a highly relevant response, make sure that requests provide\
    \ any important details or context. Otherwise you are leaving it up to the model\
    \ to guess what you mean.\n\n|||\n|---|---|\n|**Worse**|**Better**|\n|How do I\
    \ add numbers in Excel?|How do I add up a row of dollar amounts in Excel? I want\
    \ to do this automatically for a whole sheet of rows with all the totals ending\
    \ up on the right in a column called \"Total\".|\n|Who\u2019s president?|Who was\
    \ the president of Mexico in 2021, and how frequently are elections held?|\n|Write\
    \ code to calculate the Fibonacci sequence.|Write a TypeScript function to efficiently\
    \ calculate the Fibonacci sequence. Comment the code liberally to explain what\
    \ each piece does and why it's written that way.|\n|Summarize the meeting notes.|Summarize\
    \ the meeting notes in a single paragraph. Then write a markdown list of the speakers\
    \ and each of their key points. Finally, list the next steps or action items suggested\
    \ by the speakers, if any.|\n\n[\n\n#### Tactic: Ask the model to adopt a persona\n\
    \n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-to-adopt-a-persona)\n\
    \nThe system message can be used to specify the persona used by the model in its\
    \ replies.\n\nSYSTEM\n\nWhen I ask for help to write something, you will reply\
    \ with a document that contains at least one joke or playful comment in every\
    \ paragraph.\n\nUSER\n\nWrite a thank you note to my steel bolt vendor for getting\
    \ the delivery in on time and in short notice. This made it possible for us to\
    \ deliver an important order.\n\n[\n\n#### Tactic: Use delimiters to clearly indicate\
    \ distinct parts of the input\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input)\n\
    \nDelimiters like triple quotation marks, XML tags, section titles, etc. can help\
    \ demarcate sections of text to be treated differently.\n\nUSER\n\nSummarize the\
    \ text delimited by triple quotes with a haiku. \"\"\"insert text here\"\"\"\n\
    \nSYSTEM\n\nYou will be provided with a pair of articles (delimited with XML tags)\
    \ about the same topic. First summarize the arguments of each article. Then indicate\
    \ which of them makes a better argument and explain why.\n\nUSER\n\n<article>\
    \ insert first article here </article> <article> insert second article here </article>\n\
    \nSYSTEM\n\nYou will be provided with a thesis abstract and a suggested title\
    \ for it. The thesis title should give the reader a good idea of the topic of\
    \ the thesis but should also be eye-catching. If the title does not meet these\
    \ criteria, suggest 5 alternatives.\n\nUSER\n\nAbstract: insert abstract here\
    \ Title: insert title here\n\nFor straightforward tasks such as these, using delimiters\
    \ might not make a difference in the output quality. However, the more complex\
    \ a task is the more important it is to disambiguate task details. Don\u2019t\
    \ make the model work to understand exactly what you are asking of them.\n\n[\n\
    \n#### Tactic: Specify the steps required to complete a task\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-steps-required-to-complete-a-task)\n\
    \nSome tasks are best specified as a sequence of steps. Writing the steps out\
    \ explicitly can make it easier for the model to follow them.\n\nSYSTEM\n\nUse\
    \ the following step-by-step instructions to respond to user inputs. Step 1 -\
    \ The user will provide you with text in triple quotes. Summarize this text in\
    \ one sentence with a prefix that says \"Summary: \". Step 2 - Translate the summary\
    \ from Step 1 into Spanish, with a prefix that says \"Translation: \".\n\nUSER\n\
    \n\"\"\"insert text here\"\"\"\n\n[\n\n#### Tactic: Provide examples\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples)\n\
    \nProviding general instructions that apply to all examples is generally more\
    \ efficient than demonstrating all permutations of a task by example, but in some\
    \ cases providing examples may be easier. For example, if you intend for the model\
    \ to copy a particular style of responding to user queries which is difficult\
    \ to describe explicitly. This is known as \"few-shot\" prompting.\n\nSYSTEM\n\
    \nAnswer in a consistent style.\n\nUSER\n\nTeach me about patience.\n\nASSISTANT\n\
    \nThe river that carves the deepest valley flows from a modest spring; the grandest\
    \ symphony originates from a single note; the most intricate tapestry begins with\
    \ a solitary thread.\n\nUSER\n\nTeach me about the ocean.\n\n[\n\n#### Tactic:\
    \ Specify the desired length of the output\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-desired-length-of-the-output)\n\
    \nYou can ask the model to produce outputs that are of a given target length.\
    \ The targeted output length can be specified in terms of the count of words,\
    \ sentences, paragraphs, bullet points, etc. Note however that instructing the\
    \ model to generate a specific number of words does not work with high precision.\
    \ The model can more reliably generate outputs with a specific number of paragraphs\
    \ or bullet points.\n\nUSER\n\nSummarize the text delimited by triple quotes in\
    \ about 50 words. \"\"\"insert text here\"\"\"\n\nUSER\n\nSummarize the text delimited\
    \ by triple quotes in 2 paragraphs. \"\"\"insert text here\"\"\"\n\nUSER\n\nSummarize\
    \ the text delimited by triple quotes in 3 bullet points. \"\"\"insert text here\"\
    \"\"\n\n[\n\n### Strategy: Provide reference text\n\n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-provide-reference-text)\n\
    \n[\n\n#### Tactic: Instruct the model to answer using a reference text\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text)\n\
    \nIf we can provide a model with trusted information that is relevant to the current\
    \ query, then we can instruct the model to use the provided information to compose\
    \ its answer.\n\nSYSTEM\n\nUse the provided articles delimited by triple quotes\
    \ to answer questions. If the answer cannot be found in the articles, write \"\
    I could not find an answer.\"\n\nUSER\n\n<insert articles, each delimited by triple\
    \ quotes> Question: <insert question here>\n\nGiven that all models have limited\
    \ context windows, we need some way to dynamically lookup information that is\
    \ relevant to the question being asked.\_[Embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)\_\
    can be used to implement efficient knowledge retrieval. See the tactic\_[\"Use\
    \ embeddings-based search to implement efficient knowledge retrieval\"](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval)\_\
    for more details on how to implement this.\n\n[\n\n#### Tactic: Instruct the model\
    \ to answer with citations from a reference text\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-with-citations-from-a-reference-text)\n\
    \nIf the input has been supplemented with relevant knowledge, it's straightforward\
    \ to request that the model add citations to its answers by referencing passages\
    \ from provided documents. Note that citations in the output can then be verified\
    \ programmatically by string matching within the provided documents.\n\nSYSTEM\n\
    \nYou will be provided with a document delimited by triple quotes and a question.\
    \ Your task is to answer the question using only the provided document and to\
    \ cite the passage(s) of the document used to answer the question. If the document\
    \ does not contain the information needed to answer this question then simply\
    \ write: \"Insufficient information.\" If an answer to the question is provided,\
    \ it must be annotated with a citation. Use the following format for to cite relevant\
    \ passages ({\"citation\": \u2026}).\n\nUSER\n\n\"\"\"<insert document here>\"\
    \"\" Question: <insert question here>\n\n[\n\n### Strategy: Split complex tasks\
    \ into simpler subtasks\n\n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-split-complex-tasks-into-simpler-subtasks)\n\
    \n[\n\n#### Tactic: Use intent classification to identify the most relevant instructions\
    \ for a user query\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query)\n\
    \nFor tasks in which lots of independent sets of instructions are needed to handle\
    \ different cases, it can be beneficial to first classify the type of query and\
    \ to use that classification to determine which instructions are needed. This\
    \ can be achieved by defining fixed categories and hardcoding instructions that\
    \ are relevant for handling tasks in a given category. This process can also be\
    \ applied recursively to decompose a task into a sequence of stages. The advantage\
    \ of this approach is that each query will contain only those instructions that\
    \ are required to perform the next stage of a task which can result in lower error\
    \ rates compared to using a single query to perform the whole task. This can also\
    \ result in lower costs since larger prompts cost more to run ([see pricing information](https://openai.com/pricing)).\n\
    \nSuppose for example that for a customer service application, queries could be\
    \ usefully classified as follows:\n\nSYSTEM\n\nYou will be provided with customer\
    \ service queries. Classify each query into a primary category and a secondary\
    \ category. Provide your output in json format with the keys: primary and secondary.\
    \ Primary categories: Billing, Technical Support, Account Management, or General\
    \ Inquiry. Billing secondary categories: - Unsubscribe or upgrade - Add a payment\
    \ method - Explanation for charge - Dispute a charge Technical Support secondary\
    \ categories: - Troubleshooting - Device compatibility - Software updates Account\
    \ Management secondary categories: - Password reset - Update personal information\
    \ - Close account - Account security General Inquiry secondary categories: - Product\
    \ information - Pricing - Feedback - Speak to a human\n\nUSER\n\nI need to get\
    \ my internet working again.\n\nBased on the classification of the customer query,\
    \ a set of more specific instructions can be provided to a model for it to handle\
    \ next steps. For example, suppose the customer requires help with \"troubleshooting\"\
    .\n\nSYSTEM\n\nYou will be provided with customer service inquiries that require\
    \ troubleshooting in a technical support context. Help the user by: - Ask them\
    \ to check that all cables to/from the router are connected. Note that it is common\
    \ for cables to come loose over time. - If all cables are connected and the issue\
    \ persists, ask them which router model they are using - Now you will advise them\
    \ how to restart their device: -- If the model number is MTD-327J, advise them\
    \ to push the red button and hold it for 5 seconds, then wait 5 minutes before\
    \ testing the connection. -- If the model number is MTD-327S, advise them to unplug\
    \ and replug it, then wait 5 minutes before testing the connection. - If the customer's\
    \ issue persists after restarting the device and waiting 5 minutes, connect them\
    \ to IT support by outputting {\"IT support requested\"}. - If the user starts\
    \ asking questions that are unrelated to this topic then confirm if they would\
    \ like to end the current chat about troubleshooting and classify their request\
    \ according to the following scheme: <insert primary/secondary classification\
    \ scheme from above here>\n\nUSER\n\nI need to get my internet working again.\n\
    \nNotice that the model has been instructed to emit special strings to indicate\
    \ when the state of the conversation changes. This enables us to turn our system\
    \ into a state machine where the state determines which instructions are injected.\
    \ By keeping track of state, what instructions are relevant at that state, and\
    \ also optionally what state transitions are allowed from that state, we can put\
    \ guardrails around the user experience that would be hard to achieve with a less\
    \ structured approach.\n\n[\n\n#### Tactic: For dialogue applications that require\
    \ very long conversations, summarize or filter previous dialogue\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-for-dialogue-applications-that-require-very-long-conversations-summarize-or-filter-previous-dialogue)\n\
    \nSince models have a fixed context length, dialogue between a user and an assistant\
    \ in which the entire conversation is included in the context window cannot continue\
    \ indefinitely.\n\nThere are various workarounds to this problem, one of which\
    \ is to summarize previous turns in the conversation. Once the size of the input\
    \ reaches a predetermined threshold length, this could trigger a query that summarizes\
    \ part of the conversation and the summary of the prior conversation could be\
    \ included as part of the system message. Alternatively, prior conversation could\
    \ be summarized asynchronously in the background throughout the entire conversation.\n\
    \nAn alternative solution is to dynamically select previous parts of the conversation\
    \ that are most relevant to the current query. See the tactic\_[\"Use embeddings-based\
    \ search to implement efficient knowledge retrieval\"](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval).\n\
    \n[\n\n#### Tactic: Summarize long documents piecewise and construct a full summary\
    \ recursively\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-summarize-long-documents-piecewise-and-construct-a-full-summary-recursively)\n\
    \nSince models have a fixed context length, they cannot be used to summarize a\
    \ text longer than the context length minus the length of the generated summary\
    \ in a single query.\n\nTo summarize a very long document such as a book we can\
    \ use a sequence of queries to summarize each section of the document. Section\
    \ summaries can be concatenated and summarized producing summaries of summaries.\
    \ This process can proceed recursively until an entire document is summarized.\
    \ If it\u2019s necessary to use information about earlier sections in order to\
    \ make sense of later sections, then a further trick that can be useful is to\
    \ include a running summary of the text that precedes any given point in the book\
    \ while summarizing content at that point. The effectiveness of this procedure\
    \ for summarizing books has been studied in previous\_[research](https://openai.com/research/summarizing-books)\_\
    by OpenAI using variants of GPT-3.\n\n[\n\n### Strategy: Give models time to \"\
    think\"\n\n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-give-models-time-to-think)\n\
    \n[\n\n#### Tactic: Instruct the model to work out its own solution before rushing\
    \ to a conclusion\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion)\n\
    \nSometimes we get better results when we explicitly instruct the model to reason\
    \ from first principles before coming to a conclusion. Suppose for example we\
    \ want a model to evaluate a student\u2019s solution to a math problem. The most\
    \ obvious way to approach this is to simply ask the model if the student's solution\
    \ is correct or not.\n\nSYSTEM\n\nDetermine if the student's solution is correct\
    \ or not.\n\nUSER\n\nProblem Statement: I'm building a solar power installation\
    \ and I need help working out the financials. - Land costs $100 / square foot\
    \ - I can buy solar panels for $250 / square foot - I negotiated a contract for\
    \ maintenance that will cost me a flat $100k per year, and an additional $10 /\
    \ square foot What is the total cost for the first year of operations as a function\
    \ of the number of square feet. Student's Solution: Let x be the size of the installation\
    \ in square feet. 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance\
    \ cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\
    \nASSISTANT\n\nThe student's solution is correct.\n\nBut the student's solution\
    \ is actually not correct! We can get the model to successfully notice this by\
    \ prompting the model to generate its own solution first.\n\nSYSTEM\n\nFirst work\
    \ out your own solution to the problem. Then compare your solution to the student's\
    \ solution and evaluate if the student's solution is correct or not. Don't decide\
    \ if the student's solution is correct until you have done the problem yourself.\n\
    \nUSER\n\nProblem Statement: I'm building a solar power installation and I need\
    \ help working out the financials. - Land costs $100 / square foot - I can buy\
    \ solar panels for $250 / square foot - I negotiated a contract for maintenance\
    \ that will cost me a flat $100k per year, and an additional $10 / square foot\
    \ What is the total cost for the first year of operations as a function of the\
    \ number of square feet. Student's Solution: Let x be the size of the installation\
    \ in square feet. 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance\
    \ cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\
    \nASSISTANT\n\nLet x be the size of the installation in square feet. 1. Land cost:\
    \ 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 10x Total cost:\
    \ 100x + 250x + 100,000 + 10x = 360x + 100,000 The student's solution is incorrect.\
    \ They made an error in the maintenance cost calculation, using 100x instead of\
    \ 10x. The correct total cost for the first year of operations is 360x + 100,000.\n\
    \n[\n\n#### Tactic: Use inner monologue or a sequence of queries to hide the model's\
    \ reasoning process\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-inner-monologue-or-a-sequence-of-queries-to-hide-the-model-s-reasoning-process)\n\
    \nThe previous tactic demonstrates that it is sometimes important for the model\
    \ to reason in detail about a problem before answering a specific question. For\
    \ some applications, the reasoning process that a model uses to arrive at a final\
    \ answer would be inappropriate to share with the user. For example, in tutoring\
    \ applications we may want to encourage students to work out their own answers,\
    \ but a model\u2019s reasoning process about the student\u2019s solution could\
    \ reveal the answer to the student.\n\nInner monologue is a tactic that can be\
    \ used to mitigate this. The idea of inner monologue is to instruct the model\
    \ to put parts of the output that are meant to be hidden from the user into a\
    \ structured format that makes parsing them easy. Then before presenting the output\
    \ to the user, the output is parsed and only part of the output is made visible.\n\
    \nSYSTEM\n\nFollow these steps to answer the user queries. Step 1 - First work\
    \ out your own solution to the problem. Don't rely on the student's solution since\
    \ it may be incorrect. Enclose all your work for this step within triple quotes\
    \ (\"\"\"). Step 2 - Compare your solution to the student's solution and evaluate\
    \ if the student's solution is correct or not. Enclose all your work for this\
    \ step within triple quotes (\"\"\"). Step 3 - If the student made a mistake,\
    \ determine what hint you could give the student without giving away the answer.\
    \ Enclose all your work for this step within triple quotes (\"\"\"). Step 4 -\
    \ If the student made a mistake, provide the hint from the previous step to the\
    \ student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write\
    \ \"Hint:\".\n\nUSER\n\nProblem Statement: <insert problem statement> Student\
    \ Solution: <insert student solution>\n\nAlternatively, this can be achieved with\
    \ a sequence of queries in which all except the last have their output hidden\
    \ from the end user.\n\nFirst, we can ask the model to solve the problem on its\
    \ own. Since this initial query doesn't require the student\u2019s solution, it\
    \ can be omitted. This provides the additional advantage that there is no chance\
    \ that the model\u2019s solution will be biased by the student\u2019s attempted\
    \ solution.\n\nUSER\n\n<insert problem statement>\n\nNext, we can have the model\
    \ use all available information to assess the correctness of the student\u2019\
    s solution.\n\nSYSTEM\n\nCompare your solution to the student's solution and evaluate\
    \ if the student's solution is correct or not.\n\nUSER\n\nProblem statement: \"\
    \"\"<insert problem statement>\"\"\" Your solution: \"\"\"<insert model generated\
    \ solution>\"\"\" Student\u2019s solution: \"\"\"<insert student's solution>\"\
    \"\"\n\nFinally, we can let the model use its own analysis to construct a reply\
    \ in the persona of a helpful tutor.\n\nSYSTEM\n\nYou are a math tutor. If the\
    \ student made an error, offer a hint to the student in a way that does not reveal\
    \ the answer. If the student did not make an error, simply offer them an encouraging\
    \ comment.\n\nUSER\n\nProblem statement: \"\"\"<insert problem statement>\"\"\"\
    \ Your solution: \"\"\"<insert model generated solution>\"\"\" Student\u2019s\
    \ solution: \"\"\"<insert student's solution>\"\"\" Analysis: \"\"\"<insert model\
    \ generated analysis from previous step>\"\"\"\n\n[\n\n#### Tactic: Ask the model\
    \ if it missed anything on previous passes\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-if-it-missed-anything-on-previous-passes)\n\
    \nSuppose that we are using a model to list excerpts from a source which are relevant\
    \ to a particular question. After listing each excerpt the model needs to determine\
    \ if it should start writing another or if it should stop. If the source document\
    \ is large, it is common for a model to stop too early and fail to list all relevant\
    \ excerpts. In that case, better performance can often be obtained by prompting\
    \ the model with followup queries to find any excerpts it missed on previous passes.\n\
    \nSYSTEM\n\nYou will be provided with a document delimited by triple quotes. Your\
    \ task is to select excerpts which pertain to the following question: \"What significant\
    \ paradigm shifts have occurred in the history of artificial intelligence.\" Ensure\
    \ that excerpts contain all relevant context needed to interpret them - in other\
    \ words don't extract small snippets that are missing important context. Provide\
    \ output in JSON format as follows: [{\"excerpt\": \"...\"}, ... {\"excerpt\"\
    : \"...\"}]\n\nUSER\n\n\"\"\"<insert document here>\"\"\"\n\nASSISTANT\n\n[{\"\
    excerpt\": \"the model writes an excerpt here\"}, ... {\"excerpt\": \"the model\
    \ writes another excerpt here\"}]\n\nUSER\n\nAre there more relevant excerpts?\
    \ Take care not to repeat excerpts. Also ensure that excerpts contain all relevant\
    \ context needed to interpret them - in other words don't extract small snippets\
    \ that are missing important context.\n\n[\n\n### Strategy: Use external tools\n\
    \n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-use-external-tools)\n\
    \n[\n\n#### Tactic: Use embeddings-based search to implement efficient knowledge\
    \ retrieval\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval)\n\
    \nA model can leverage external sources of information if provided as part of\
    \ its input. This can help the model to generate more informed and up-to-date\
    \ responses. For example, if a user asks a question about a specific movie, it\
    \ may be useful to add high quality information about the movie (e.g. actors,\
    \ director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement\
    \ efficient knowledge retrieval, so that relevant information can be added to\
    \ the model input dynamically at run-time.\n\nA text embedding is a vector that\
    \ can measure the relatedness between text strings. Similar or relevant strings\
    \ will be closer together than unrelated strings. This fact, along with the existence\
    \ of fast vector search algorithms means that embeddings can be used to implement\
    \ efficient knowledge retrieval. In particular, a text corpus can be split up\
    \ into chunks, and each chunk can be embedded and stored. Then a given query can\
    \ be embedded and vector search can be performed to find the embedded chunks of\
    \ text from the corpus that are most related to the query (i.e. closest together\
    \ in the embedding space).\n\nExample implementations can be found in the\_[OpenAI\
    \ Cookbook](https://cookbook.openai.com/examples/vector_databases/readme). See\
    \ the tactic\_[\u201CInstruct the model to use retrieved knowledge to answer queries\u201D\
    ](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text)\_\
    for an example of how to use knowledge retrieval to minimize the likelihood that\
    \ a model will make up incorrect facts.\n\n[\n\n#### Tactic: Use code execution\
    \ to perform more accurate calculations or call external APIs\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-code-execution-to-perform-more-accurate-calculations-or-call-external-apis)\n\
    \nLanguage models cannot be relied upon to perform arithmetic or long calculations\
    \ accurately on their own. In cases where this is needed, a model can be instructed\
    \ to write and run code instead of making its own calculations. In particular,\
    \ a model can be instructed to put code that is meant to be run into a designated\
    \ format such as triple backtick. After an output is produced, the code can be\
    \ extracted and run. Finally, if necessary, the output from the code execution\
    \ engine (i.e. Python interpreter) can be provided as an input to the model for\
    \ the next query.\n\nSYSTEM\n\nYou can write and execute Python code by enclosing\
    \ it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\n\
    \nUSER\n\nFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4\
    \ - 3*x**3 - 7*x - 10.\n\nAnother good use case for code execution is calling\
    \ external APIs. If a model is instructed in the proper use of an API, it can\
    \ write code that makes use of it. A model can be instructed in how to use an\
    \ API by providing it with documentation and/or code samples showing how to use\
    \ the API.\n\nSYSTEM\n\nYou can write and execute Python code by enclosing it\
    \ in triple backticks. Also note that you have access to the following module\
    \ to help users send messages to their friends: ```python import message message.write(to=\"\
    John\", message=\"Hey, want to meetup after work?\")```\n\n**WARNING: Executing\
    \ code produced by a model is not inherently safe and precautions should be taken\
    \ in any application that seeks to do this. In particular, a sandboxed code execution\
    \ environment is needed to limit the harm that untrusted code could cause.**\n\
    \n[\n\n#### Tactic: Give the model access to specific functions\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-give-the-model-access-to-specific-functions)\n\
    \nThe Chat Completions API allows passing a list of function descriptions in requests.\
    \ This enables models to generate function arguments according to the provided\
    \ schemas. Generated function arguments are returned by the API in JSON format\
    \ and can be used to execute function calls. Output provided by function calls\
    \ can then be fed back into a model in the following request to close the loop.\
    \ This is the recommended way of using OpenAI models to call external functions.\
    \ To learn more see the\_[function calling section](https://platform.openai.com/docs/guides/function-calling)\_\
    in our introductory text generation guide and more\_[function calling examples](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)\_\
    in the OpenAI Cookbook.\n\n[\n\n### Strategy: Test changes systematically\n\n\
    ](https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically)\n\
    \nSometimes it can be hard to tell whether a change \u2014 e.g., a new instruction\
    \ or a new design \u2014 makes your system better or worse. Looking at a few examples\
    \ may hint at which is better, but with small sample sizes it can be hard to distinguish\
    \ between a true improvement or random luck. Maybe the change helps performance\
    \ on some inputs, but hurts performance on others.\n\nEvaluation procedures (or\
    \ \"evals\") are useful for optimizing system designs. Good evals are:\n\n- Representative\
    \ of real-world usage (or at least diverse)\n- Contain many test cases for greater\
    \ statistical power (see table below for guidelines)\n- Easy to automate or repeat\n\
    \n|Difference to detect|Sample size needed for 95% confidence|\n|---|---|\n|30%|~10|\n\
    |10%|~100|\n|3%|~1,000|\n|1%|~10,000|\n\nEvaluation of outputs can be done by\
    \ computers, humans, or a mix. Computers can automate evals with objective criteria\
    \ (e.g., questions with single correct answers) as well as some subjective or\
    \ fuzzy criteria, in which model outputs are evaluated by other model queries.\_\
    [OpenAI Evals](https://github.com/openai/evals)\_is an open-source software framework\
    \ that provides tools for creating automated evals.\n\nModel-based evals can be\
    \ useful when there exists a range of possible outputs that would be considered\
    \ equally high in quality (e.g. for questions with long answers). The boundary\
    \ between what can be realistically evaluated with a model-based eval and what\
    \ requires a human to evaluate is fuzzy and is constantly shifting as models become\
    \ more capable. We encourage experimentation to figure out how well model-based\
    \ evals can work for your use case.\n\n[\n\n#### Tactic: Evaluate model outputs\
    \ with reference to gold-standard answers\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-evaluate-model-outputs-with-reference-to-gold-standard-answers)\n\
    \nSuppose it is known that the correct answer to a question should make reference\
    \ to a specific set of known facts. Then we can use a model query to count how\
    \ many of the required facts are included in the answer.\n\nFor example, using\
    \ the following system message:\n\nSYSTEM\n\nYou will be provided with text delimited\
    \ by triple quotes that is supposed to be the answer to a question. Check if the\
    \ following pieces of information are directly contained in the answer: - Neil\
    \ Armstrong was the first person to walk on the moon. - The date Neil Armstrong\
    \ first walked on the moon was July 21, 1969. For each of these points perform\
    \ the following steps: 1 - Restate the point. 2 - Provide a citation from the\
    \ answer which is closest to this point. 3 - Consider if someone reading the citation\
    \ who doesn't know the topic could directly infer the point. Explain why or why\
    \ not before making up your mind. 4 - Write \"yes\" if the answer to 3 was yes,\
    \ otherwise write \"no\". Finally, provide a count of how many \"yes\" answers\
    \ there are. Provide this count as {\"count\": <insert count here>}.\n\nHere's\
    \ an example input where both points are satisfied:\n\nSYSTEM\n\n<insert system\
    \ message above>\n\nUSER\n\n\"\"\"Neil Armstrong is famous for being the first\
    \ human to set foot on the Moon. This historic event took place on July 21, 1969,\
    \ during the Apollo 11 mission.\"\"\"\n\nHere's an example input where only one\
    \ point is satisfied:\n\nSYSTEM\n\n<insert system message above>\n\nUSER\n\n\"\
    \"\"Neil Armstrong made history when he stepped off the lunar module, becoming\
    \ the first person to walk on the moon.\"\"\"\n\nHere's an example input where\
    \ none are satisfied:\n\nSYSTEM\n\n<insert system message above>\n\nUSER\n\n\"\
    \"\"In the summer of '69, a voyage grand, Apollo 11, bold as legend's hand. Armstrong\
    \ took a step, history unfurled, \"One small step,\" he said, for a new world.\"\
    \"\"\n\nThere are many possible variants on this type of model-based eval. Consider\
    \ the following variation which tracks the kind of overlap between the candidate\
    \ answer and the gold-standard answer, and also tracks whether the candidate answer\
    \ contradicts any part of the gold-standard answer.\n\nSYSTEM\n\nUse the following\
    \ steps to respond to user inputs. Fully restate each step before proceeding.\
    \ i.e. \"Step 1: Reason...\". Step 1: Reason step-by-step about whether the information\
    \ in the submitted answer compared to the expert answer is either: disjoint, equal,\
    \ a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\
    \ Step 2: Reason step-by-step about whether the submitted answer contradicts any\
    \ aspect of the expert answer. Step 3: Output a JSON object structured like: {\"\
    type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or\
    \ \"overlapping\", \"contradiction\": true or false}\n\nHere's an example input\
    \ with a substandard answer which nonetheless does not contradict the expert answer:\n\
    \nSYSTEM\n\n<insert system message above>\n\nUSER\n\nQuestion: \"\"\"What event\
    \ is Neil Armstrong most famous for and on what date did it occur? Assume UTC\
    \ time.\"\"\" Submitted Answer: \"\"\"Didn't he walk on the moon or something?\"\
    \"\" Expert Answer: \"\"\"Neil Armstrong is most famous for being the first person\
    \ to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"\n\n\
    Here's an example input with answer that directly contradicts the expert answer:\n\
    \nSYSTEM\n\n<insert system message above>\n\nUSER\n\nQuestion: \"\"\"What event\
    \ is Neil Armstrong most famous for and on what date did it occur? Assume UTC\
    \ time.\"\"\" Submitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong\
    \ became the second person to walk on the moon, following after Buzz Aldrin.\"\
    \"\" Expert Answer: \"\"\"Neil Armstrong is most famous for being the first person\
    \ to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"\n\n\
    Here's an example input with a correct answer that also provides a bit more detail\
    \ than is necessary:\n\nSYSTEM\n\n<insert system message above>\n\nUSER\n\nQuestion:\
    \ \"\"\"What event is Neil Armstrong most famous for and on what date did it occur?\
    \ Assume UTC time.\"\"\" Submitted Answer: \"\"\"At approximately 02:56 UTC on\
    \ July 21st 1969, Neil Armstrong became the first human to set foot on the lunar\
    \ surface, marking a monumental achievement in human history.\"\"\" Expert Answer:\
    \ \"\"\"Neil Armstrong is most famous for being the first person to walk on the\
    \ moon. This historic event occurred on July 21, 1969.\"\"\"\n\n[\n\n## Other\
    \ resources\n\n](https://platform.openai.com/docs/guides/prompt-engineering/other-resources)\n\
    \nFor more inspiration, visit the\_[OpenAI Cookbook](https://cookbook.openai.com/),\
    \ which contains example code and also links to third-party resources such as:\n\
    \n- [Prompting libraries & tools](https://cookbook.openai.com/related_resources#prompting-libraries--tools)\n\
    - [Prompting guides](https://cookbook.openai.com/related_resources#prompting-guides)\n\
    - [Video courses](https://cookbook.openai.com/related_resources#video-courses)\n\
    - [Papers on advanced prompting to improve reasoning](https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning)}}"
  destination_file: null
  prompt_file: Prompts/Extract Tips.md
  source_file: null
  timestamp: '2024-09-11T10:34:09.125129'
- command: ask  --prompt=None --template=None
  custom_prompt: "Read the entire text and identify tips, techniques, recommendations,\
    \ and insights. Classify these into sections as headings. List these within the\
    \ relevant sections using numbered list, ranking by order of importance in each\
    \ section. Each item should have a short phrase in bold introducing the item and\
    \ one sentence describing it.\n\n{{[\n\n# Prompt engineering\n\n](https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering)\n\
    \nThis guide shares strategies and tactics for getting better results from large\
    \ language models (sometimes referred to as GPT models) like GPT-4o. The methods\
    \ described here can sometimes be deployed in combination for greater effect.\
    \ We encourage experimentation to find the methods that work best for you.\n\n\
    You can also explore example prompts which showcase what our models are capable\
    \ of:\n\n[\n\nPrompt examples\n\nExplore prompt examples to learn what GPT models\
    \ can do\n\n\n\n\n\n\n\n](https://platform.openai.com/examples)\n\n[\n\n## Six\
    \ strategies for getting better results\n\n](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results)\n\
    \n[\n\n### Write clear instructions\n\n](https://platform.openai.com/docs/guides/prompt-engineering/write-clear-instructions)\n\
    \nThese models can\u2019t read your mind. If outputs are too long, ask for brief\
    \ replies. If outputs are too simple, ask for expert-level writing. If you dislike\
    \ the format, demonstrate the format you\u2019d like to see. The less the model\
    \ has to guess at what you want, the more likely you\u2019ll get it.\n\nTactics:\n\
    \n- [Include details in your query to get more relevant answers](https://platform.openai.com/docs/guides/prompt-engineering/tactic-include-details-in-your-query-to-get-more-relevant-answers)\n\
    - [Ask the model to adopt a persona](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-to-adopt-a-persona)\n\
    - [Use delimiters to clearly indicate distinct parts of the input](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input)\n\
    - [Specify the steps required to complete a task](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-steps-required-to-complete-a-task)\n\
    - [Provide examples](https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples)\n\
    - [Specify the desired length of the output](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-desired-length-of-the-output)\n\
    \n[\n\n### Provide reference text\n\n](https://platform.openai.com/docs/guides/prompt-engineering/provide-reference-text)\n\
    \nLanguage models can confidently invent fake answers, especially when asked about\
    \ esoteric topics or for citations and URLs. In the same way that a sheet of notes\
    \ can help a student do better on a test, providing reference text to these models\
    \ can help in answering with fewer fabrications.\n\nTactics:\n\n- [Instruct the\
    \ model to answer using a reference text](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text)\n\
    - [Instruct the model to answer with citations from a reference text](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-with-citations-from-a-reference-text)\n\
    \n[\n\n### Split complex tasks into simpler subtasks\n\n](https://platform.openai.com/docs/guides/prompt-engineering/split-complex-tasks-into-simpler-subtasks)\n\
    \nJust as it is good practice in software engineering to decompose a complex system\
    \ into a set of modular components, the same is true of tasks submitted to a language\
    \ model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore,\
    \ complex tasks can often be re-defined as a workflow of simpler tasks in which\
    \ the outputs of earlier tasks are used to construct the inputs to later tasks.\n\
    \nTactics:\n\n- [Use intent classification to identify the most relevant instructions\
    \ for a user query](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query)\n\
    - [For dialogue applications that require very long conversations, summarize or\
    \ filter previous dialogue](https://platform.openai.com/docs/guides/prompt-engineering/tactic-for-dialogue-applications-that-require-very-long-conversations-summarize-or-filter-previous-dialogue)\n\
    - [Summarize long documents piecewise and construct a full summary recursively](https://platform.openai.com/docs/guides/prompt-engineering/tactic-summarize-long-documents-piecewise-and-construct-a-full-summary-recursively)\n\
    \n[\n\n### Give the model time to \"think\"\n\n](https://platform.openai.com/docs/guides/prompt-engineering/give-the-model-time-to-think)\n\
    \nIf asked to multiply 17 by 28, you might not know it instantly, but can still\
    \ work it out with time. Similarly, models make more reasoning errors when trying\
    \ to answer right away, rather than taking time to work out an answer. Asking\
    \ for a \"chain of thought\" before an answer can help the model reason its way\
    \ toward correct answers more reliably.\n\nTactics:\n\n- [Instruct the model to\
    \ work out its own solution before rushing to a conclusion](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion)\n\
    - [Use inner monologue or a sequence of queries to hide the model's reasoning\
    \ process](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-inner-monologue-or-a-sequence-of-queries-to-hide-the-model-s-reasoning-process)\n\
    - [Ask the model if it missed anything on previous passes](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-if-it-missed-anything-on-previous-passes)\n\
    \n[\n\n### Use external tools\n\n](https://platform.openai.com/docs/guides/prompt-engineering/use-external-tools)\n\
    \nCompensate for the weaknesses of the model by feeding it the outputs of other\
    \ tools. For example, a text retrieval system (sometimes called RAG or retrieval\
    \ augmented generation) can tell the model about relevant documents. A code execution\
    \ engine like OpenAI's Code Interpreter can help the model do math and run code.\
    \ If a task can be done more reliably or efficiently by a tool rather than by\
    \ a language model, offload it to get the best of both.\n\nTactics:\n\n- [Use\
    \ embeddings-based search to implement efficient knowledge retrieval](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval)\n\
    - [Use code execution to perform more accurate calculations or call external APIs](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-code-execution-to-perform-more-accurate-calculations-or-call-external-apis)\n\
    - [Give the model access to specific functions](https://platform.openai.com/docs/guides/prompt-engineering/tactic-give-the-model-access-to-specific-functions)\n\
    \n[\n\n### Test changes systematically\n\n](https://platform.openai.com/docs/guides/prompt-engineering/test-changes-systematically)\n\
    \nImproving performance is easier if you can measure it. In some cases a modification\
    \ to a prompt will achieve better performance on a few isolated examples but lead\
    \ to worse overall performance on a more representative set of examples. Therefore\
    \ to be sure that a change is net positive to performance it may be necessary\
    \ to define a comprehensive test suite (also known an as an \"eval\").\n\nTactic:\n\
    \n- [Evaluate model outputs with reference to gold-standard answers](https://platform.openai.com/docs/guides/prompt-engineering/tactic-evaluate-model-outputs-with-reference-to-gold-standard-answers)\n\
    \n[\n\n## Tactics\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactics)\n\
    \nEach of the strategies listed above can be instantiated with specific tactics.\
    \ These tactics are meant to provide ideas for things to try. They are by no means\
    \ fully comprehensive, and you should feel free to try creative ideas not represented\
    \ here.\n\n[\n\n### Strategy: Write clear instructions\n\n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions)\n\
    \n[\n\n#### Tactic: Include details in your query to get more relevant answers\n\
    \n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-include-details-in-your-query-to-get-more-relevant-answers)\n\
    \nIn order to get a highly relevant response, make sure that requests provide\
    \ any important details or context. Otherwise you are leaving it up to the model\
    \ to guess what you mean.\n\n|||\n|---|---|\n|**Worse**|**Better**|\n|How do I\
    \ add numbers in Excel?|How do I add up a row of dollar amounts in Excel? I want\
    \ to do this automatically for a whole sheet of rows with all the totals ending\
    \ up on the right in a column called \"Total\".|\n|Who\u2019s president?|Who was\
    \ the president of Mexico in 2021, and how frequently are elections held?|\n|Write\
    \ code to calculate the Fibonacci sequence.|Write a TypeScript function to efficiently\
    \ calculate the Fibonacci sequence. Comment the code liberally to explain what\
    \ each piece does and why it's written that way.|\n|Summarize the meeting notes.|Summarize\
    \ the meeting notes in a single paragraph. Then write a markdown list of the speakers\
    \ and each of their key points. Finally, list the next steps or action items suggested\
    \ by the speakers, if any.|\n\n[\n\n#### Tactic: Ask the model to adopt a persona\n\
    \n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-to-adopt-a-persona)\n\
    \nThe system message can be used to specify the persona used by the model in its\
    \ replies.\n\nSYSTEM\n\nWhen I ask for help to write something, you will reply\
    \ with a document that contains at least one joke or playful comment in every\
    \ paragraph.\n\nUSER\n\nWrite a thank you note to my steel bolt vendor for getting\
    \ the delivery in on time and in short notice. This made it possible for us to\
    \ deliver an important order.\n\n[\n\n#### Tactic: Use delimiters to clearly indicate\
    \ distinct parts of the input\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input)\n\
    \nDelimiters like triple quotation marks, XML tags, section titles, etc. can help\
    \ demarcate sections of text to be treated differently.\n\nUSER\n\nSummarize the\
    \ text delimited by triple quotes with a haiku. \"\"\"insert text here\"\"\"\n\
    \nSYSTEM\n\nYou will be provided with a pair of articles (delimited with XML tags)\
    \ about the same topic. First summarize the arguments of each article. Then indicate\
    \ which of them makes a better argument and explain why.\n\nUSER\n\n<article>\
    \ insert first article here </article> <article> insert second article here </article>\n\
    \nSYSTEM\n\nYou will be provided with a thesis abstract and a suggested title\
    \ for it. The thesis title should give the reader a good idea of the topic of\
    \ the thesis but should also be eye-catching. If the title does not meet these\
    \ criteria, suggest 5 alternatives.\n\nUSER\n\nAbstract: insert abstract here\
    \ Title: insert title here\n\nFor straightforward tasks such as these, using delimiters\
    \ might not make a difference in the output quality. However, the more complex\
    \ a task is the more important it is to disambiguate task details. Don\u2019t\
    \ make the model work to understand exactly what you are asking of them.\n\n[\n\
    \n#### Tactic: Specify the steps required to complete a task\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-steps-required-to-complete-a-task)\n\
    \nSome tasks are best specified as a sequence of steps. Writing the steps out\
    \ explicitly can make it easier for the model to follow them.\n\nSYSTEM\n\nUse\
    \ the following step-by-step instructions to respond to user inputs. Step 1 -\
    \ The user will provide you with text in triple quotes. Summarize this text in\
    \ one sentence with a prefix that says \"Summary: \". Step 2 - Translate the summary\
    \ from Step 1 into Spanish, with a prefix that says \"Translation: \".\n\nUSER\n\
    \n\"\"\"insert text here\"\"\"\n\n[\n\n#### Tactic: Provide examples\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples)\n\
    \nProviding general instructions that apply to all examples is generally more\
    \ efficient than demonstrating all permutations of a task by example, but in some\
    \ cases providing examples may be easier. For example, if you intend for the model\
    \ to copy a particular style of responding to user queries which is difficult\
    \ to describe explicitly. This is known as \"few-shot\" prompting.\n\nSYSTEM\n\
    \nAnswer in a consistent style.\n\nUSER\n\nTeach me about patience.\n\nASSISTANT\n\
    \nThe river that carves the deepest valley flows from a modest spring; the grandest\
    \ symphony originates from a single note; the most intricate tapestry begins with\
    \ a solitary thread.\n\nUSER\n\nTeach me about the ocean.\n\n[\n\n#### Tactic:\
    \ Specify the desired length of the output\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-desired-length-of-the-output)\n\
    \nYou can ask the model to produce outputs that are of a given target length.\
    \ The targeted output length can be specified in terms of the count of words,\
    \ sentences, paragraphs, bullet points, etc. Note however that instructing the\
    \ model to generate a specific number of words does not work with high precision.\
    \ The model can more reliably generate outputs with a specific number of paragraphs\
    \ or bullet points.\n\nUSER\n\nSummarize the text delimited by triple quotes in\
    \ about 50 words. \"\"\"insert text here\"\"\"\n\nUSER\n\nSummarize the text delimited\
    \ by triple quotes in 2 paragraphs. \"\"\"insert text here\"\"\"\n\nUSER\n\nSummarize\
    \ the text delimited by triple quotes in 3 bullet points. \"\"\"insert text here\"\
    \"\"\n\n[\n\n### Strategy: Provide reference text\n\n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-provide-reference-text)\n\
    \n[\n\n#### Tactic: Instruct the model to answer using a reference text\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text)\n\
    \nIf we can provide a model with trusted information that is relevant to the current\
    \ query, then we can instruct the model to use the provided information to compose\
    \ its answer.\n\nSYSTEM\n\nUse the provided articles delimited by triple quotes\
    \ to answer questions. If the answer cannot be found in the articles, write \"\
    I could not find an answer.\"\n\nUSER\n\n<insert articles, each delimited by triple\
    \ quotes> Question: <insert question here>\n\nGiven that all models have limited\
    \ context windows, we need some way to dynamically lookup information that is\
    \ relevant to the question being asked.\_[Embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)\_\
    can be used to implement efficient knowledge retrieval. See the tactic\_[\"Use\
    \ embeddings-based search to implement efficient knowledge retrieval\"](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval)\_\
    for more details on how to implement this.\n\n[\n\n#### Tactic: Instruct the model\
    \ to answer with citations from a reference text\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-with-citations-from-a-reference-text)\n\
    \nIf the input has been supplemented with relevant knowledge, it's straightforward\
    \ to request that the model add citations to its answers by referencing passages\
    \ from provided documents. Note that citations in the output can then be verified\
    \ programmatically by string matching within the provided documents.\n\nSYSTEM\n\
    \nYou will be provided with a document delimited by triple quotes and a question.\
    \ Your task is to answer the question using only the provided document and to\
    \ cite the passage(s) of the document used to answer the question. If the document\
    \ does not contain the information needed to answer this question then simply\
    \ write: \"Insufficient information.\" If an answer to the question is provided,\
    \ it must be annotated with a citation. Use the following format for to cite relevant\
    \ passages ({\"citation\": \u2026}).\n\nUSER\n\n\"\"\"<insert document here>\"\
    \"\" Question: <insert question here>\n\n[\n\n### Strategy: Split complex tasks\
    \ into simpler subtasks\n\n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-split-complex-tasks-into-simpler-subtasks)\n\
    \n[\n\n#### Tactic: Use intent classification to identify the most relevant instructions\
    \ for a user query\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query)\n\
    \nFor tasks in which lots of independent sets of instructions are needed to handle\
    \ different cases, it can be beneficial to first classify the type of query and\
    \ to use that classification to determine which instructions are needed. This\
    \ can be achieved by defining fixed categories and hardcoding instructions that\
    \ are relevant for handling tasks in a given category. This process can also be\
    \ applied recursively to decompose a task into a sequence of stages. The advantage\
    \ of this approach is that each query will contain only those instructions that\
    \ are required to perform the next stage of a task which can result in lower error\
    \ rates compared to using a single query to perform the whole task. This can also\
    \ result in lower costs since larger prompts cost more to run ([see pricing information](https://openai.com/pricing)).\n\
    \nSuppose for example that for a customer service application, queries could be\
    \ usefully classified as follows:\n\nSYSTEM\n\nYou will be provided with customer\
    \ service queries. Classify each query into a primary category and a secondary\
    \ category. Provide your output in json format with the keys: primary and secondary.\
    \ Primary categories: Billing, Technical Support, Account Management, or General\
    \ Inquiry. Billing secondary categories: - Unsubscribe or upgrade - Add a payment\
    \ method - Explanation for charge - Dispute a charge Technical Support secondary\
    \ categories: - Troubleshooting - Device compatibility - Software updates Account\
    \ Management secondary categories: - Password reset - Update personal information\
    \ - Close account - Account security General Inquiry secondary categories: - Product\
    \ information - Pricing - Feedback - Speak to a human\n\nUSER\n\nI need to get\
    \ my internet working again.\n\nBased on the classification of the customer query,\
    \ a set of more specific instructions can be provided to a model for it to handle\
    \ next steps. For example, suppose the customer requires help with \"troubleshooting\"\
    .\n\nSYSTEM\n\nYou will be provided with customer service inquiries that require\
    \ troubleshooting in a technical support context. Help the user by: - Ask them\
    \ to check that all cables to/from the router are connected. Note that it is common\
    \ for cables to come loose over time. - If all cables are connected and the issue\
    \ persists, ask them which router model they are using - Now you will advise them\
    \ how to restart their device: -- If the model number is MTD-327J, advise them\
    \ to push the red button and hold it for 5 seconds, then wait 5 minutes before\
    \ testing the connection. -- If the model number is MTD-327S, advise them to unplug\
    \ and replug it, then wait 5 minutes before testing the connection. - If the customer's\
    \ issue persists after restarting the device and waiting 5 minutes, connect them\
    \ to IT support by outputting {\"IT support requested\"}. - If the user starts\
    \ asking questions that are unrelated to this topic then confirm if they would\
    \ like to end the current chat about troubleshooting and classify their request\
    \ according to the following scheme: <insert primary/secondary classification\
    \ scheme from above here>\n\nUSER\n\nI need to get my internet working again.\n\
    \nNotice that the model has been instructed to emit special strings to indicate\
    \ when the state of the conversation changes. This enables us to turn our system\
    \ into a state machine where the state determines which instructions are injected.\
    \ By keeping track of state, what instructions are relevant at that state, and\
    \ also optionally what state transitions are allowed from that state, we can put\
    \ guardrails around the user experience that would be hard to achieve with a less\
    \ structured approach.\n\n[\n\n#### Tactic: For dialogue applications that require\
    \ very long conversations, summarize or filter previous dialogue\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-for-dialogue-applications-that-require-very-long-conversations-summarize-or-filter-previous-dialogue)\n\
    \nSince models have a fixed context length, dialogue between a user and an assistant\
    \ in which the entire conversation is included in the context window cannot continue\
    \ indefinitely.\n\nThere are various workarounds to this problem, one of which\
    \ is to summarize previous turns in the conversation. Once the size of the input\
    \ reaches a predetermined threshold length, this could trigger a query that summarizes\
    \ part of the conversation and the summary of the prior conversation could be\
    \ included as part of the system message. Alternatively, prior conversation could\
    \ be summarized asynchronously in the background throughout the entire conversation.\n\
    \nAn alternative solution is to dynamically select previous parts of the conversation\
    \ that are most relevant to the current query. See the tactic\_[\"Use embeddings-based\
    \ search to implement efficient knowledge retrieval\"](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval).\n\
    \n[\n\n#### Tactic: Summarize long documents piecewise and construct a full summary\
    \ recursively\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-summarize-long-documents-piecewise-and-construct-a-full-summary-recursively)\n\
    \nSince models have a fixed context length, they cannot be used to summarize a\
    \ text longer than the context length minus the length of the generated summary\
    \ in a single query.\n\nTo summarize a very long document such as a book we can\
    \ use a sequence of queries to summarize each section of the document. Section\
    \ summaries can be concatenated and summarized producing summaries of summaries.\
    \ This process can proceed recursively until an entire document is summarized.\
    \ If it\u2019s necessary to use information about earlier sections in order to\
    \ make sense of later sections, then a further trick that can be useful is to\
    \ include a running summary of the text that precedes any given point in the book\
    \ while summarizing content at that point. The effectiveness of this procedure\
    \ for summarizing books has been studied in previous\_[research](https://openai.com/research/summarizing-books)\_\
    by OpenAI using variants of GPT-3.\n\n[\n\n### Strategy: Give models time to \"\
    think\"\n\n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-give-models-time-to-think)\n\
    \n[\n\n#### Tactic: Instruct the model to work out its own solution before rushing\
    \ to a conclusion\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion)\n\
    \nSometimes we get better results when we explicitly instruct the model to reason\
    \ from first principles before coming to a conclusion. Suppose for example we\
    \ want a model to evaluate a student\u2019s solution to a math problem. The most\
    \ obvious way to approach this is to simply ask the model if the student's solution\
    \ is correct or not.\n\nSYSTEM\n\nDetermine if the student's solution is correct\
    \ or not.\n\nUSER\n\nProblem Statement: I'm building a solar power installation\
    \ and I need help working out the financials. - Land costs $100 / square foot\
    \ - I can buy solar panels for $250 / square foot - I negotiated a contract for\
    \ maintenance that will cost me a flat $100k per year, and an additional $10 /\
    \ square foot What is the total cost for the first year of operations as a function\
    \ of the number of square feet. Student's Solution: Let x be the size of the installation\
    \ in square feet. 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance\
    \ cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\
    \nASSISTANT\n\nThe student's solution is correct.\n\nBut the student's solution\
    \ is actually not correct! We can get the model to successfully notice this by\
    \ prompting the model to generate its own solution first.\n\nSYSTEM\n\nFirst work\
    \ out your own solution to the problem. Then compare your solution to the student's\
    \ solution and evaluate if the student's solution is correct or not. Don't decide\
    \ if the student's solution is correct until you have done the problem yourself.\n\
    \nUSER\n\nProblem Statement: I'm building a solar power installation and I need\
    \ help working out the financials. - Land costs $100 / square foot - I can buy\
    \ solar panels for $250 / square foot - I negotiated a contract for maintenance\
    \ that will cost me a flat $100k per year, and an additional $10 / square foot\
    \ What is the total cost for the first year of operations as a function of the\
    \ number of square feet. Student's Solution: Let x be the size of the installation\
    \ in square feet. 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance\
    \ cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\
    \nASSISTANT\n\nLet x be the size of the installation in square feet. 1. Land cost:\
    \ 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 10x Total cost:\
    \ 100x + 250x + 100,000 + 10x = 360x + 100,000 The student's solution is incorrect.\
    \ They made an error in the maintenance cost calculation, using 100x instead of\
    \ 10x. The correct total cost for the first year of operations is 360x + 100,000.\n\
    \n[\n\n#### Tactic: Use inner monologue or a sequence of queries to hide the model's\
    \ reasoning process\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-inner-monologue-or-a-sequence-of-queries-to-hide-the-model-s-reasoning-process)\n\
    \nThe previous tactic demonstrates that it is sometimes important for the model\
    \ to reason in detail about a problem before answering a specific question. For\
    \ some applications, the reasoning process that a model uses to arrive at a final\
    \ answer would be inappropriate to share with the user. For example, in tutoring\
    \ applications we may want to encourage students to work out their own answers,\
    \ but a model\u2019s reasoning process about the student\u2019s solution could\
    \ reveal the answer to the student.\n\nInner monologue is a tactic that can be\
    \ used to mitigate this. The idea of inner monologue is to instruct the model\
    \ to put parts of the output that are meant to be hidden from the user into a\
    \ structured format that makes parsing them easy. Then before presenting the output\
    \ to the user, the output is parsed and only part of the output is made visible.\n\
    \nSYSTEM\n\nFollow these steps to answer the user queries. Step 1 - First work\
    \ out your own solution to the problem. Don't rely on the student's solution since\
    \ it may be incorrect. Enclose all your work for this step within triple quotes\
    \ (\"\"\"). Step 2 - Compare your solution to the student's solution and evaluate\
    \ if the student's solution is correct or not. Enclose all your work for this\
    \ step within triple quotes (\"\"\"). Step 3 - If the student made a mistake,\
    \ determine what hint you could give the student without giving away the answer.\
    \ Enclose all your work for this step within triple quotes (\"\"\"). Step 4 -\
    \ If the student made a mistake, provide the hint from the previous step to the\
    \ student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write\
    \ \"Hint:\".\n\nUSER\n\nProblem Statement: <insert problem statement> Student\
    \ Solution: <insert student solution>\n\nAlternatively, this can be achieved with\
    \ a sequence of queries in which all except the last have their output hidden\
    \ from the end user.\n\nFirst, we can ask the model to solve the problem on its\
    \ own. Since this initial query doesn't require the student\u2019s solution, it\
    \ can be omitted. This provides the additional advantage that there is no chance\
    \ that the model\u2019s solution will be biased by the student\u2019s attempted\
    \ solution.\n\nUSER\n\n<insert problem statement>\n\nNext, we can have the model\
    \ use all available information to assess the correctness of the student\u2019\
    s solution.\n\nSYSTEM\n\nCompare your solution to the student's solution and evaluate\
    \ if the student's solution is correct or not.\n\nUSER\n\nProblem statement: \"\
    \"\"<insert problem statement>\"\"\" Your solution: \"\"\"<insert model generated\
    \ solution>\"\"\" Student\u2019s solution: \"\"\"<insert student's solution>\"\
    \"\"\n\nFinally, we can let the model use its own analysis to construct a reply\
    \ in the persona of a helpful tutor.\n\nSYSTEM\n\nYou are a math tutor. If the\
    \ student made an error, offer a hint to the student in a way that does not reveal\
    \ the answer. If the student did not make an error, simply offer them an encouraging\
    \ comment.\n\nUSER\n\nProblem statement: \"\"\"<insert problem statement>\"\"\"\
    \ Your solution: \"\"\"<insert model generated solution>\"\"\" Student\u2019s\
    \ solution: \"\"\"<insert student's solution>\"\"\" Analysis: \"\"\"<insert model\
    \ generated analysis from previous step>\"\"\"\n\n[\n\n#### Tactic: Ask the model\
    \ if it missed anything on previous passes\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-if-it-missed-anything-on-previous-passes)\n\
    \nSuppose that we are using a model to list excerpts from a source which are relevant\
    \ to a particular question. After listing each excerpt the model needs to determine\
    \ if it should start writing another or if it should stop. If the source document\
    \ is large, it is common for a model to stop too early and fail to list all relevant\
    \ excerpts. In that case, better performance can often be obtained by prompting\
    \ the model with followup queries to find any excerpts it missed on previous passes.\n\
    \nSYSTEM\n\nYou will be provided with a document delimited by triple quotes. Your\
    \ task is to select excerpts which pertain to the following question: \"What significant\
    \ paradigm shifts have occurred in the history of artificial intelligence.\" Ensure\
    \ that excerpts contain all relevant context needed to interpret them - in other\
    \ words don't extract small snippets that are missing important context. Provide\
    \ output in JSON format as follows: [{\"excerpt\": \"...\"}, ... {\"excerpt\"\
    : \"...\"}]\n\nUSER\n\n\"\"\"<insert document here>\"\"\"\n\nASSISTANT\n\n[{\"\
    excerpt\": \"the model writes an excerpt here\"}, ... {\"excerpt\": \"the model\
    \ writes another excerpt here\"}]\n\nUSER\n\nAre there more relevant excerpts?\
    \ Take care not to repeat excerpts. Also ensure that excerpts contain all relevant\
    \ context needed to interpret them - in other words don't extract small snippets\
    \ that are missing important context.\n\n[\n\n### Strategy: Use external tools\n\
    \n](https://platform.openai.com/docs/guides/prompt-engineering/strategy-use-external-tools)\n\
    \n[\n\n#### Tactic: Use embeddings-based search to implement efficient knowledge\
    \ retrieval\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval)\n\
    \nA model can leverage external sources of information if provided as part of\
    \ its input. This can help the model to generate more informed and up-to-date\
    \ responses. For example, if a user asks a question about a specific movie, it\
    \ may be useful to add high quality information about the movie (e.g. actors,\
    \ director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement\
    \ efficient knowledge retrieval, so that relevant information can be added to\
    \ the model input dynamically at run-time.\n\nA text embedding is a vector that\
    \ can measure the relatedness between text strings. Similar or relevant strings\
    \ will be closer together than unrelated strings. This fact, along with the existence\
    \ of fast vector search algorithms means that embeddings can be used to implement\
    \ efficient knowledge retrieval. In particular, a text corpus can be split up\
    \ into chunks, and each chunk can be embedded and stored. Then a given query can\
    \ be embedded and vector search can be performed to find the embedded chunks of\
    \ text from the corpus that are most related to the query (i.e. closest together\
    \ in the embedding space).\n\nExample implementations can be found in the\_[OpenAI\
    \ Cookbook](https://cookbook.openai.com/examples/vector_databases/readme). See\
    \ the tactic\_[\u201CInstruct the model to use retrieved knowledge to answer queries\u201D\
    ](https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text)\_\
    for an example of how to use knowledge retrieval to minimize the likelihood that\
    \ a model will make up incorrect facts.\n\n[\n\n#### Tactic: Use code execution\
    \ to perform more accurate calculations or call external APIs\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-code-execution-to-perform-more-accurate-calculations-or-call-external-apis)\n\
    \nLanguage models cannot be relied upon to perform arithmetic or long calculations\
    \ accurately on their own. In cases where this is needed, a model can be instructed\
    \ to write and run code instead of making its own calculations. In particular,\
    \ a model can be instructed to put code that is meant to be run into a designated\
    \ format such as triple backtick. After an output is produced, the code can be\
    \ extracted and run. Finally, if necessary, the output from the code execution\
    \ engine (i.e. Python interpreter) can be provided as an input to the model for\
    \ the next query.\n\nSYSTEM\n\nYou can write and execute Python code by enclosing\
    \ it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\n\
    \nUSER\n\nFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4\
    \ - 3*x**3 - 7*x - 10.\n\nAnother good use case for code execution is calling\
    \ external APIs. If a model is instructed in the proper use of an API, it can\
    \ write code that makes use of it. A model can be instructed in how to use an\
    \ API by providing it with documentation and/or code samples showing how to use\
    \ the API.\n\nSYSTEM\n\nYou can write and execute Python code by enclosing it\
    \ in triple backticks. Also note that you have access to the following module\
    \ to help users send messages to their friends: ```python import message message.write(to=\"\
    John\", message=\"Hey, want to meetup after work?\")```\n\n**WARNING: Executing\
    \ code produced by a model is not inherently safe and precautions should be taken\
    \ in any application that seeks to do this. In particular, a sandboxed code execution\
    \ environment is needed to limit the harm that untrusted code could cause.**\n\
    \n[\n\n#### Tactic: Give the model access to specific functions\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-give-the-model-access-to-specific-functions)\n\
    \nThe Chat Completions API allows passing a list of function descriptions in requests.\
    \ This enables models to generate function arguments according to the provided\
    \ schemas. Generated function arguments are returned by the API in JSON format\
    \ and can be used to execute function calls. Output provided by function calls\
    \ can then be fed back into a model in the following request to close the loop.\
    \ This is the recommended way of using OpenAI models to call external functions.\
    \ To learn more see the\_[function calling section](https://platform.openai.com/docs/guides/function-calling)\_\
    in our introductory text generation guide and more\_[function calling examples](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)\_\
    in the OpenAI Cookbook.\n\n[\n\n### Strategy: Test changes systematically\n\n\
    ](https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically)\n\
    \nSometimes it can be hard to tell whether a change \u2014 e.g., a new instruction\
    \ or a new design \u2014 makes your system better or worse. Looking at a few examples\
    \ may hint at which is better, but with small sample sizes it can be hard to distinguish\
    \ between a true improvement or random luck. Maybe the change helps performance\
    \ on some inputs, but hurts performance on others.\n\nEvaluation procedures (or\
    \ \"evals\") are useful for optimizing system designs. Good evals are:\n\n- Representative\
    \ of real-world usage (or at least diverse)\n- Contain many test cases for greater\
    \ statistical power (see table below for guidelines)\n- Easy to automate or repeat\n\
    \n|Difference to detect|Sample size needed for 95% confidence|\n|---|---|\n|30%|~10|\n\
    |10%|~100|\n|3%|~1,000|\n|1%|~10,000|\n\nEvaluation of outputs can be done by\
    \ computers, humans, or a mix. Computers can automate evals with objective criteria\
    \ (e.g., questions with single correct answers) as well as some subjective or\
    \ fuzzy criteria, in which model outputs are evaluated by other model queries.\_\
    [OpenAI Evals](https://github.com/openai/evals)\_is an open-source software framework\
    \ that provides tools for creating automated evals.\n\nModel-based evals can be\
    \ useful when there exists a range of possible outputs that would be considered\
    \ equally high in quality (e.g. for questions with long answers). The boundary\
    \ between what can be realistically evaluated with a model-based eval and what\
    \ requires a human to evaluate is fuzzy and is constantly shifting as models become\
    \ more capable. We encourage experimentation to figure out how well model-based\
    \ evals can work for your use case.\n\n[\n\n#### Tactic: Evaluate model outputs\
    \ with reference to gold-standard answers\n\n](https://platform.openai.com/docs/guides/prompt-engineering/tactic-evaluate-model-outputs-with-reference-to-gold-standard-answers)\n\
    \nSuppose it is known that the correct answer to a question should make reference\
    \ to a specific set of known facts. Then we can use a model query to count how\
    \ many of the required facts are included in the answer.\n\nFor example, using\
    \ the following system message:\n\nSYSTEM\n\nYou will be provided with text delimited\
    \ by triple quotes that is supposed to be the answer to a question. Check if the\
    \ following pieces of information are directly contained in the answer: - Neil\
    \ Armstrong was the first person to walk on the moon. - The date Neil Armstrong\
    \ first walked on the moon was July 21, 1969. For each of these points perform\
    \ the following steps: 1 - Restate the point. 2 - Provide a citation from the\
    \ answer which is closest to this point. 3 - Consider if someone reading the citation\
    \ who doesn't know the topic could directly infer the point. Explain why or why\
    \ not before making up your mind. 4 - Write \"yes\" if the answer to 3 was yes,\
    \ otherwise write \"no\". Finally, provide a count of how many \"yes\" answers\
    \ there are. Provide this count as {\"count\": <insert count here>}.\n\nHere's\
    \ an example input where both points are satisfied:\n\nSYSTEM\n\n<insert system\
    \ message above>\n\nUSER\n\n\"\"\"Neil Armstrong is famous for being the first\
    \ human to set foot on the Moon. This historic event took place on July 21, 1969,\
    \ during the Apollo 11 mission.\"\"\"\n\nHere's an example input where only one\
    \ point is satisfied:\n\nSYSTEM\n\n<insert system message above>\n\nUSER\n\n\"\
    \"\"Neil Armstrong made history when he stepped off the lunar module, becoming\
    \ the first person to walk on the moon.\"\"\"\n\nHere's an example input where\
    \ none are satisfied:\n\nSYSTEM\n\n<insert system message above>\n\nUSER\n\n\"\
    \"\"In the summer of '69, a voyage grand, Apollo 11, bold as legend's hand. Armstrong\
    \ took a step, history unfurled, \"One small step,\" he said, for a new world.\"\
    \"\"\n\nThere are many possible variants on this type of model-based eval. Consider\
    \ the following variation which tracks the kind of overlap between the candidate\
    \ answer and the gold-standard answer, and also tracks whether the candidate answer\
    \ contradicts any part of the gold-standard answer.\n\nSYSTEM\n\nUse the following\
    \ steps to respond to user inputs. Fully restate each step before proceeding.\
    \ i.e. \"Step 1: Reason...\". Step 1: Reason step-by-step about whether the information\
    \ in the submitted answer compared to the expert answer is either: disjoint, equal,\
    \ a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\
    \ Step 2: Reason step-by-step about whether the submitted answer contradicts any\
    \ aspect of the expert answer. Step 3: Output a JSON object structured like: {\"\
    type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or\
    \ \"overlapping\", \"contradiction\": true or false}\n\nHere's an example input\
    \ with a substandard answer which nonetheless does not contradict the expert answer:\n\
    \nSYSTEM\n\n<insert system message above>\n\nUSER\n\nQuestion: \"\"\"What event\
    \ is Neil Armstrong most famous for and on what date did it occur? Assume UTC\
    \ time.\"\"\" Submitted Answer: \"\"\"Didn't he walk on the moon or something?\"\
    \"\" Expert Answer: \"\"\"Neil Armstrong is most famous for being the first person\
    \ to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"\n\n\
    Here's an example input with answer that directly contradicts the expert answer:\n\
    \nSYSTEM\n\n<insert system message above>\n\nUSER\n\nQuestion: \"\"\"What event\
    \ is Neil Armstrong most famous for and on what date did it occur? Assume UTC\
    \ time.\"\"\" Submitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong\
    \ became the second person to walk on the moon, following after Buzz Aldrin.\"\
    \"\" Expert Answer: \"\"\"Neil Armstrong is most famous for being the first person\
    \ to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"\n\n\
    Here's an example input with a correct answer that also provides a bit more detail\
    \ than is necessary:\n\nSYSTEM\n\n<insert system message above>\n\nUSER\n\nQuestion:\
    \ \"\"\"What event is Neil Armstrong most famous for and on what date did it occur?\
    \ Assume UTC time.\"\"\" Submitted Answer: \"\"\"At approximately 02:56 UTC on\
    \ July 21st 1969, Neil Armstrong became the first human to set foot on the lunar\
    \ surface, marking a monumental achievement in human history.\"\"\" Expert Answer:\
    \ \"\"\"Neil Armstrong is most famous for being the first person to walk on the\
    \ moon. This historic event occurred on July 21, 1969.\"\"\"\n\n[\n\n## Other\
    \ resources\n\n](https://platform.openai.com/docs/guides/prompt-engineering/other-resources)\n\
    \nFor more inspiration, visit the\_[OpenAI Cookbook](https://cookbook.openai.com/),\
    \ which contains example code and also links to third-party resources such as:\n\
    \n- [Prompting libraries & tools](https://cookbook.openai.com/related_resources#prompting-libraries--tools)\n\
    - [Prompting guides](https://cookbook.openai.com/related_resources#prompting-guides)\n\
    - [Video courses](https://cookbook.openai.com/related_resources#video-courses)\n\
    - [Papers on advanced prompting to improve reasoning](https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning)}}"
  destination_file: Posts/writing-clear-instructions-include-details-queries-provide-important-context-specifics.md
  prompt_file: Prompts/Extract Tips.md
  source_file: null
  timestamp: '2024-09-11T10:35:26.070424'
